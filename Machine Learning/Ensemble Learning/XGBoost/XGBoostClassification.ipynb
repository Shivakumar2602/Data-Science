{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XGBoostClassification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmFJ5fYqSAoHvGYlvTKVz3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P5-3laU_ONwy"},"source":["#XG Boost Classification"]},{"cell_type":"code","metadata":{"id":"izzYr-mLOIUU","executionInfo":{"status":"ok","timestamp":1605046656574,"user_tz":360,"elapsed":1825,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}}},"source":["from sklearn import datasets\n","import xgboost as xgb\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"PO2AOKuhPHrX","executionInfo":{"status":"ok","timestamp":1605046667224,"user_tz":360,"elapsed":491,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J3YIJbk7PL-r"},"source":["In order for XGBoost to be able to use our data, we’ll need to transform it into a specific format that XGBoost can handle. That format is called DMatrix. It’s a very simple one-linear to transform a numpy array of data to DMatrix format:"]},{"cell_type":"code","metadata":{"id":"aTvcgl0gPKzE","executionInfo":{"status":"ok","timestamp":1605046679826,"user_tz":360,"elapsed":511,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}}},"source":["D_train = xgb.DMatrix(X_train, label=Y_train)\n","D_test = xgb.DMatrix(X_test, label=Y_test)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dW_kToLyPgP-"},"source":["Defining an XGBoost model\n","Now that our data is all loaded up, we can define the parameters of our gradient boosting ensemble. We’ve set up some of the most important ones below to get us started. For more complicated tasks and models, the full list of possible parameters is available on the official [XGBoost website](https://xgboost.readthedocs.io/en/latest/parameter.html)"]},{"cell_type":"code","metadata":{"id":"cOHhdHuMRO0g","executionInfo":{"status":"ok","timestamp":1605047220597,"user_tz":360,"elapsed":657,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}}},"source":["param = {\n","    'eta': 0.3, \n","    'max_depth': 3,  \n","    'objective': 'multi:softprob',  \n","    'num_class': 3} \n","\n","steps = 20  # The number of training iterations"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwpA9ywGRV1k"},"source":["The simplest parameters are the max_depth (maximum depth of the decision trees being trained), objective (the loss function being used), and num_class (the number of classes in the dataset). The eta algorithm requires special attention.\n","\n","From our theory, Gradient Boosting involves creating and adding decision trees to an ensemble model sequentially. New trees are created to correct the residual errors in the predictions from the existing ensemble.\n","\n","Due to the nature of an ensemble, i.e having several models put together to form what is essentially a very large complicated one, makes this technique prone to overfitting. The eta parameter gives us a chance to prevent this overfitting\n","\n","The eta can be thought of more intuitively as a learning rate. Rather than simply adding the predictions of new trees to the ensemble with full weight, the eta will be multiplied by the residuals being adding to reduce their weight. This effectively reduces the complexity of the overall model.\n","\n","It is common to have small values in the range of 0.1 to 0.3. The smaller weighting of these residuals will still help us train a powerful model, but won’t let that model run away into deep complexity where overfitting is more likely to happen."]},{"cell_type":"code","metadata":{"id":"TIpMby9GRoa-","executionInfo":{"status":"ok","timestamp":1605047345116,"user_tz":360,"elapsed":601,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}}},"source":["#We can finally train our model similar to how we do so with Scikit Learn:\n","model = xgb.train(param, D_train, steps)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSxgtsCnRyt3"},"source":["Let’s now run an evaluation. Again the process is very similar to that of training models in Scikit Learn:"]},{"cell_type":"code","metadata":{"id":"ohmT7tXHRQH7","executionInfo":{"status":"ok","timestamp":1605047378603,"user_tz":360,"elapsed":645,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}},"outputId":"b4f52eef-65a7-43a0-914f-4f29da1c1136","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","from sklearn.metrics import precision_score, recall_score, accuracy_score\n","\n","preds = model.predict(D_test)\n","best_preds = np.asarray([np.argmax(line) for line in preds])\n","\n","print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n","print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n","print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Precision = 0.9629629629629629\n","Recall = 0.9696969696969697\n","Accuracy = 0.9666666666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QOEkrXT_SYHq"},"source":["**Further Exploration with XGBoost**\n","\n","That just about sums up the basics of XGBoost. But there are some more cool features that’ll help you get the most out of your models.\n","\n","* The gamma parameter can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.\n","* The booster parameter allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.\n","* Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily:"]},{"cell_type":"code","metadata":{"id":"Eyrh5P3GSnlk","executionInfo":{"status":"ok","timestamp":1605047742943,"user_tz":360,"elapsed":156438,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}},"outputId":"7c53fdc5-d025-4a98-e8f2-f51e2db781ab","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.model_selection import GridSearchCV\n","\n","clf = xgb.XGBClassifier()\n","parameters = {\n","     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n","     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n","     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n","     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n","     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n","     }\n","\n","grid = GridSearchCV(clf,\n","                    parameters, n_jobs=4,\n","                    scoring=\"neg_log_loss\",\n","                    cv=3)\n","\n","grid.fit(X_train, Y_train)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=3, error_score=nan,\n","             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n","                                     colsample_bylevel=1, colsample_bynode=1,\n","                                     colsample_bytree=1, gamma=0,\n","                                     learning_rate=0.1, max_delta_step=0,\n","                                     max_depth=3, min_child_weight=1,\n","                                     missing=None, n_estimators=100, n_jobs=1,\n","                                     nthread=None, objective='binary:logistic',\n","                                     random_state=0, reg_alpha=0, reg_lambda=1,\n","                                     scale_po...ight=1, seed=None, silent=None,\n","                                     subsample=1, verbosity=1),\n","             iid='deprecated', n_jobs=4,\n","             param_grid={'colsample_bytree': [0.3, 0.4, 0.5, 0.7],\n","                         'eta': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n","                         'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n","                         'max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n","                         'min_child_weight': [1, 3, 5, 7]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring='neg_log_loss', verbose=0)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"WVt3uYl6UOaL","executionInfo":{"status":"ok","timestamp":1605048016960,"user_tz":360,"elapsed":374,"user":{"displayName":"Shiva kumar Ramavath","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjiYRpt3IW9-bN_N32tPhfgKdgzds55Fy2ns03cAg=s64","userId":"05600410476591999105"}},"outputId":"d3371db3-45c2-41f1-a27a-1a9c6d3aba9f","colab":{"base_uri":"https://localhost:8080/"}},"source":["best_accuracy = grid.best_score_\n","best_parameters = grid.best_params_\n","print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n","print(\"Best Parameters:\", best_parameters)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Best Accuracy: -15.64 %\n","Best Parameters: {'colsample_bytree': 0.5, 'eta': 0.05, 'gamma': 0.3, 'max_depth': 6, 'min_child_weight': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eTdE8K6gTCBx"},"source":["Link: https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7 \n","open link in incognito mode"]}]}